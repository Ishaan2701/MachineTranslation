{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CzXqYwtrZwuJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "PATH = \"/content/Hindi_English_Truncated_Corpus.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QXIxgE5ZZ9aT"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "\n",
    "def hindi_preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MAzRuRbyaHMA"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path=PATH):\n",
    "    lines=pd.read_csv(path,encoding='utf-8')\n",
    "    lines=lines.dropna()\n",
    "    lines = lines[lines['source']=='ted']\n",
    "    en = []\n",
    "    hd = []\n",
    "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
    "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
    "        en_1.append('<end>')\n",
    "        en_1.insert(0, '<start>')\n",
    "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
    "        hd_1.append('<end>')\n",
    "        hd_1.insert(0, '<start>')\n",
    "        en.append(en_1)\n",
    "        hd.append(hd_1)\n",
    "    return hd, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZTSl11DwaKJS"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bOLR6EVJaMUe"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ENf_Uz9KaOOw"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path=PATH):\n",
    "    targ_lang, inp_lang = create_dataset(path)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uLXdJ4Isb8Vo"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnracH-4cIwZ",
    "outputId": "ad537d86-97bb-47d1-d5c9-ca2368eb54bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: kaggle: command not found\r\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download aiswaryaramachandran/hindienglish-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1HIDe6zciJd",
    "outputId": "861c1388-fc68-4a14-86d6-f571b2547771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  hindienglish-corpora.zip\n",
      "  inflating: Hindi_English_Truncated_Corpus.csv  \n"
     ]
    }
   ],
   "source": [
    "! unzip hindienglish-corpora.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xjEioT1-aQXQ"
   },
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(\"/Users/ishaanbuch/Desktop/Hindi_English_Truncated_Corpus.csv\")\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_KuZ-bvaTMD",
    "outputId": "76442334-f34b-46f6-ac2a-a1df7c6dd383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31904 31904 7977 7977\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmTg-oXBc0pa",
    "outputId": "cfcf8f63-e5d0-4579-cb21-d03d3e2629b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "54 ----> which\n",
      "11 ----> is\n",
      "3 ----> the\n",
      "17115 ----> best known\n",
      "5174 ----> treatment\n",
      "20 ----> for\n",
      "3504 ----> malaria .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "20 ----> जो\n",
      "10 ----> कि\n",
      "1516 ----> मलेरिया\n",
      "14 ----> का\n",
      "101 ----> सबसे\n",
      "892 ----> बेहतरीन\n",
      "859 ----> इलाज\n",
      "36 ----> है।\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "    \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7NYFmAGtc6X3"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZmdruhKCdWJB"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LvccTRT7dW1d"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l1O58xeVdb8I"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UgCwKkoFdcr4"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#   print(type(mask))\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UaRuR5Xqdey1"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "v6qgCocUdg6d"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    # Teacher forcing\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))      \n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYKdRavOdjSo",
    "outputId": "0a657b57-0c72-4959-ee1c-ddf77792e7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.1501\n",
      "Epoch 1 Batch 100 Loss 2.0858\n",
      "Epoch 1 Batch 200 Loss 1.7675\n",
      "Epoch 1 Batch 300 Loss 1.8358\n",
      "Epoch 1 Batch 400 Loss 1.7831\n",
      "Epoch 1 Loss 1.9420\n",
      "Time taken for 1 epoch 591.6396231651306 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.9479\n",
      "Epoch 2 Batch 100 Loss 1.7175\n",
      "Epoch 2 Batch 200 Loss 1.6219\n",
      "Epoch 2 Batch 300 Loss 1.7412\n",
      "Epoch 2 Batch 400 Loss 1.6616\n",
      "Epoch 2 Loss 1.7317\n",
      "Time taken for 1 epoch 550.5097279548645 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5519\n",
      "Epoch 3 Batch 100 Loss 1.6254\n",
      "Epoch 3 Batch 200 Loss 1.6175\n",
      "Epoch 3 Batch 300 Loss 1.6196\n",
      "Epoch 3 Batch 400 Loss 1.6264\n",
      "Epoch 3 Loss 1.6228\n",
      "Time taken for 1 epoch 550.3620147705078 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.7071\n",
      "Epoch 4 Batch 100 Loss 1.6501\n",
      "Epoch 4 Batch 200 Loss 1.4781\n",
      "Epoch 4 Batch 300 Loss 1.4270\n",
      "Epoch 4 Batch 400 Loss 1.5424\n",
      "Epoch 4 Loss 1.5303\n",
      "Time taken for 1 epoch 549.2138810157776 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.3391\n",
      "Epoch 5 Batch 100 Loss 1.3796\n",
      "Epoch 5 Batch 200 Loss 1.6572\n",
      "Epoch 5 Batch 300 Loss 1.4437\n",
      "Epoch 5 Batch 400 Loss 1.4919\n",
      "Epoch 5 Loss 1.4365\n",
      "Time taken for 1 epoch 540.2229108810425 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.4219\n",
      "Epoch 6 Batch 100 Loss 1.3745\n",
      "Epoch 6 Batch 200 Loss 1.3322\n",
      "Epoch 6 Batch 300 Loss 1.4924\n",
      "Epoch 6 Batch 400 Loss 1.3306\n",
      "Epoch 6 Loss 1.3471\n",
      "Time taken for 1 epoch 549.0495181083679 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2897\n",
      "Epoch 7 Batch 100 Loss 1.3094\n",
      "Epoch 7 Batch 200 Loss 1.1630\n",
      "Epoch 7 Batch 300 Loss 1.2155\n",
      "Epoch 7 Batch 400 Loss 1.2651\n",
      "Epoch 7 Loss 1.2619\n",
      "Time taken for 1 epoch 549.7485508918762 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2274\n",
      "Epoch 8 Batch 100 Loss 1.0612\n",
      "Epoch 8 Batch 200 Loss 1.1962\n",
      "Epoch 8 Batch 300 Loss 1.0481\n",
      "Epoch 8 Batch 400 Loss 1.2177\n",
      "Epoch 8 Loss 1.1786\n",
      "Time taken for 1 epoch 549.6073889732361 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.9020\n",
      "Epoch 9 Batch 100 Loss 0.9382\n",
      "Epoch 9 Batch 200 Loss 1.0922\n",
      "Epoch 9 Batch 300 Loss 1.0952\n",
      "Epoch 9 Batch 400 Loss 1.0336\n",
      "Epoch 9 Loss 1.0988\n",
      "Time taken for 1 epoch 545.9099750518799 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.0320\n",
      "Epoch 10 Batch 100 Loss 0.9546\n",
      "Epoch 10 Batch 200 Loss 1.0132\n",
      "Epoch 10 Batch 300 Loss 1.1320\n",
      "Epoch 10 Batch 400 Loss 1.0293\n",
      "Epoch 10 Loss 1.0226\n",
      "Time taken for 1 epoch 546.5662767887115 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.8930\n",
      "Epoch 11 Batch 100 Loss 1.0152\n",
      "Epoch 11 Batch 200 Loss 1.0585\n",
      "Epoch 11 Batch 300 Loss 0.9775\n",
      "Epoch 11 Batch 400 Loss 0.9304\n",
      "Epoch 11 Loss 0.9490\n",
      "Time taken for 1 epoch 543.7427849769592 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.8233\n",
      "Epoch 12 Batch 100 Loss 0.8977\n",
      "Epoch 12 Batch 200 Loss 0.7591\n",
      "Epoch 12 Batch 300 Loss 0.7795\n",
      "Epoch 12 Batch 400 Loss 0.8012\n",
      "Epoch 12 Loss 0.8792\n",
      "Time taken for 1 epoch 544.4316530227661 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.7835\n",
      "Epoch 13 Batch 100 Loss 0.9011\n",
      "Epoch 13 Batch 200 Loss 0.8224\n",
      "Epoch 13 Batch 300 Loss 0.8936\n",
      "Epoch 13 Batch 400 Loss 0.8422\n",
      "Epoch 13 Loss 0.8131\n",
      "Time taken for 1 epoch 546.0556111335754 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6707\n",
      "Epoch 14 Batch 100 Loss 0.7487\n",
      "Epoch 14 Batch 200 Loss 0.6549\n",
      "Epoch 14 Batch 300 Loss 0.8733\n",
      "Epoch 14 Batch 400 Loss 0.6757\n",
      "Epoch 14 Loss 0.7533\n",
      "Time taken for 1 epoch 545.0590589046478 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.6173\n",
      "Epoch 15 Batch 100 Loss 0.7320\n",
      "Epoch 15 Batch 200 Loss 0.7867\n",
      "Epoch 15 Batch 300 Loss 0.7150\n",
      "Epoch 15 Batch 400 Loss 0.7619\n",
      "Epoch 15 Loss 0.7007\n",
      "Time taken for 1 epoch 544.4002487659454 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.6543\n",
      "Epoch 16 Batch 100 Loss 0.6562\n",
      "Epoch 16 Batch 200 Loss 0.7277\n",
      "Epoch 16 Batch 300 Loss 0.6113\n",
      "Epoch 16 Batch 400 Loss 0.6327\n",
      "Epoch 16 Loss 0.6525\n",
      "Time taken for 1 epoch 544.7504560947418 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5511\n",
      "Epoch 17 Batch 100 Loss 0.6029\n",
      "Epoch 17 Batch 200 Loss 0.6052\n",
      "Epoch 17 Batch 300 Loss 0.6174\n",
      "Epoch 17 Batch 400 Loss 0.6985\n",
      "Epoch 17 Loss 0.6049\n",
      "Time taken for 1 epoch 544.9351918697357 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.4996\n",
      "Epoch 18 Batch 100 Loss 0.5291\n",
      "Epoch 18 Batch 200 Loss 0.5521\n",
      "Epoch 18 Batch 300 Loss 0.5952\n",
      "Epoch 18 Batch 400 Loss 0.6257\n",
      "Epoch 18 Loss 0.5639\n",
      "Time taken for 1 epoch 544.3320949077606 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5035\n",
      "Epoch 19 Batch 100 Loss 0.4846\n",
      "Epoch 19 Batch 200 Loss 0.4980\n",
      "Epoch 19 Batch 300 Loss 0.5463\n",
      "Epoch 19 Batch 400 Loss 0.5496\n",
      "Epoch 19 Loss 0.5252\n",
      "Time taken for 1 epoch 540.7607102394104 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5254\n",
      "Epoch 20 Batch 100 Loss 0.4751\n",
      "Epoch 20 Batch 200 Loss 0.4064\n",
      "Epoch 20 Batch 300 Loss 0.4957\n",
      "Epoch 20 Batch 400 Loss 0.5234\n",
      "Epoch 20 Loss 0.4899\n",
      "Time taken for 1 epoch 543.5931031703949 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iRrXlgMrdo_2"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z3jh035ddr9H"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oX7LFwlmdsc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: politicians do not have permission to do what needs to be done .\n",
      "Predicted translation: राजनीतिज्ञों के लिए - <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'politicians do not have permission to do what needs to be done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CC_IA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
